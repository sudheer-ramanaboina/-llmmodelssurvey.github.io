# Notes on Large Language Models

These notes form a concise introductory course on Large Language models.

The compiled version is available [here](https://harsha-jampana.github.io/llmmodelssurvey.github.io/).


A Large Language Model (LLM) uses a structure called the "transformer" to understand and produce language similar to how humans do. This type of machine learning is trained on huge amounts of text, helping it to get a grip on language patterns and structures.

Transformers, which were introduced in 2017, are a newer way to handle language data compared to the older recurrent neural networks. They're more efficient and are better at understanding the context of words in sentences.



The notes are written in Markdown and are compiled into HTML using Jekyll. Please add your changes directly to the Markdown source code. In order to install jekyll, you can follow the instructions posted on their website (https://jekyllrb.com/docs/installation/). 

Note that jekyll is only supported on GNU/Linux, Unix, or macOS. Thus, if you run Windows 10 on your local machine, you will have to install Bash on Ubuntu on Windows. Windows gives instructions on how to do that <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">here</a> and Jekyll's <a href="https://jekyllrb.com/docs/windows/">website</a> offers helpful instructions on how to proceed through the rest of the process.
